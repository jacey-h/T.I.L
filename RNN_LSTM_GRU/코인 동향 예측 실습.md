# 실습

- 코드 분석하기!!!

pyupbit 모듈 : 업비트 API를 파이썬에서 쉽게 사용하기 위해서 저자들이 개발한 모듈

```python
import pyupbit
import numpy as np
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import LSTM, Dropout, Dense, Activation
import datetime
import pandas as pd
import time
print(pyupbit.get_tickers(fiat="KRW"))
```

import datetime : 날짜와 시간처리하는 파이썬 내장 모듈

**get_tickers** 함수 : 

업비트에서 거래할 수 있는 모든 암호화폐의 문자열 ticker를 리스트로 반환

주로 이용하는 원화 마켓의 암호화폐를 선별하고 싶다면 다음과 같이 옵션을 추가해주어야 함

print(pyupbit.get_tickers(fiat="KRW"))

```python
date = None
dfs = []
for i in range(40):
    df = pyupbit.get_ohlcv("KRW-XRP",to=date, interval="minute240")
    dfs.append(df)
    date = df.index[0]
    time.sleep(0.1)
    
df = pd.concat(dfs).sort_index()
print(df)
```

**get_ohlcv** 함수 : 

인자에 ticker를 넘겨주면 해당 암호화폐의 OHLCV 데이터를 pandas DataFrame으로 반환

**시가 / 고가 / 저가 / 종가 / 거래량 / 거래금액**

open / high / low / close / volume / value

XRP : 리플코인의 화폐 단위

매개변수로 ticker, interval, count, to, period가 존재하며, 반환 값으로 Union[DataFrame, None]을 갖는다.

-interval : **interval 변수는 조회 단위를 나타낸다.** 

분(1/3/5/10/15/30/60/240), 일, 주, 월별 데이터를 구할 수 있음

예시) 

[ day / minute1 / minute3 / minute5 / minute10 / minute15 / minute30 / minute60 / minute240 / week / month ]

minute240 이니까 4시간마다로 조회한다

-count : **count 변수는 조회 개수를 나타낸다.** 

최근 영업일부터 이전 count만큼의 영업일까지의 데이터를 가져오게 된다.

count를 지정하지 않는다면 기본값은 200이다.

지정안했으니까 디폴트 값 200개를 조회

-to : **to 변수는 입력된 시점의 이전까지의 데이터를 얻을 수 있다.** 

예를 들어 to="20210101"이라고 하면, 2020년 12월 31일부터 이전 count만큼의 영업일까지의 데이터를 가져온다. 즉, 2020-06-15~2020-12-31의 데이터를 가져온다.

to를 지정하지 않는다면 기본값은 None이며, 이는 현재 일부터 가져오게 된다.

date가 None이고 이를 to의 값으로 줬으니 현재일부터 가져오도록한다

200*40 이니까 8000개

time.sleep(0.1) : 0.1초 기다리기

**pd.concat() 함수 : dateframe 합치는 방법**

![Untitled](https://user-images.githubusercontent.com/81483791/171208704-b95a96a6-0c00-4760-9250-af7ec03a72e6.png)

4시간 간격으로 현재 17일 부터 이전 8000개의 데이터

```python
high_prices = df['high']
low_prices = df['low']
mid_prices = (high_prices + low_prices) / 2
print(mid_prices[0:1349])
```

설명에서 처럼 ✓ 주식 가격은 단순히 ‘최고가’와 ‘최저가’의 평균으로 산정하였음.

```python
seq_len = 50
sequence_length = seq_len + 1

result = []
for index in range(len(mid_prices) - sequence_length):  #range(8000-51 =7949) 0~7948까지
    result.append(mid_prices[index: index + sequence_length]) 
print(result[0][:])
```

시퀀스 길이 51

![a5](https://user-images.githubusercontent.com/81483791/171208726-9137af62-feea-4117-a328-f86842bf4808.png)

result.append(mid_prices[index: index + sequence_length])

index가 0일때 0부터51까지

index가 7948일때 0부터 7999까지 (즉, 8000번 마지막꺼는 안씀)

**자세한 코드 첨부 안함**

![Untitled 1](https://user-images.githubusercontent.com/81483791/171208776-702e4af0-51d4-473e-9186-469ace36cad0.png)

result는 51개씩 7949개

첫번째꺼로 나누고 -1하니까 무조건 맨처음꺼는 0임

round함수 :  반올림 함수

**round(number [, ndigits])**

row = int(round(result.shape[0] * 0.9))  → 이거하면 7154 나옴

즉, row =7154

train = result[:row, :]  아! 훈련셋이 90퍼 이고 테스트셋이 10퍼인거

1. lstm변경 1

```python
model = Sequential()

model.add(LSTM(50, return_sequences=True, input_shape=(50,1)))

model.add(LSTM(64, return_sequences=True))
model.add(LSTM(78, return_sequences=True))

model.add(LSTM(90, return_sequences=False))

model.add(Dense(1, activation='ReLU'))

model.compile(loss='mse', optimizer='adam')

model.summary()
```

2개의 층 추가 ,  메모리셀 개수 변화

dense에서 활성화함수 relu로 변경
![변경후1](https://user-images.githubusercontent.com/81483791/171208807-75d1f634-84e9-49a3-b601-4936876f66a1.png)


활성화 함수 바꿔서 모델 만들어서 결과보기

relu, softmax, sigmoid

1. lstm 변경2

```python
model = Sequential()

model.add(LSTM(50, return_sequences=True, input_shape=(50,1)))

model.add(LSTM(64, return_sequences=False))

model.add(Dense(1, activation='ReLU'))

model.compile(loss='mse', optimizer='adam')

model.summary()
```

층 변화 없이 relu 사용하기
![변경후2](https://user-images.githubusercontent.com/81483791/171208836-777abe20-69f6-4b57-ba4e-e6ce9660c99f.png)


1. 변경3

```python
model = Sequential()

model.add(LSTM(50, return_sequences=True, input_shape=(50,1)))

model.add(LSTM(64, return_sequences=False))

model.add(Dense(1, activation='softmax'))

model.compile(loss='mse', optimizer='adam')

model.summary()
```

층 변화 없이 softmax
![변경후3](https://user-images.githubusercontent.com/81483791/171208864-e0d82f97-671b-4d95-a260-301cbd1568bf.png)

1. 변경 4

```python
model = Sequential()

model.add(LSTM(50, return_sequences=True, input_shape=(50,1)))

model.add(LSTM(64, return_sequences=False))

model.add(Dense(1, activation='sigmoid'))

model.compile(loss='mse', optimizer='adam')

model.summary()
```

층 변화 없이 sigmoid
![변경후4](https://user-images.githubusercontent.com/81483791/171208881-e4c29a20-f6b1-41a5-b616-4ae3f5816c8f.png)


1. 변경5

```python
model = Sequential()

model.add(LSTM(50, return_sequences=True, input_shape=(50,1)))

model.add(LSTM(64, return_sequences=False))

model.add(Dense(1, activation='tanh'))

model.compile(loss='mse', optimizer='adam')

model.summary()
```

층 변화 없이 tanh
![변경후5](https://user-images.githubusercontent.com/81483791/171208900-5a7cf68c-e5f3-4b99-b94d-1550ca35f85e.png)


1. 변경6

```python
model = Sequential()

model.add(LSTM(50, return_sequences=True, input_shape=(50,1)))

model.add(LSTM(64, return_sequences=True))

model.add(LSTM(80, return_sequences=False))

model.add(Dense(1, activation='tanh'))

model.compile(loss='mse', optimizer='adam')

model.summary()
```

한층 추가

![변경후6](https://user-images.githubusercontent.com/81483791/171208915-7fca0fcc-1402-4a49-bb05-b0b7ca50b524.png)

## 높은 정확도를 보이는 이유?

1. LSTM 네트워크 모델

아마존의 2013년 부터 2018년까지 일일 주가를 train data로
2019년 데이터를 test data로 사용     
![image](https://user-images.githubusercontent.com/81483791/171209032-bc48a207-41f9-404d-b7a6-d073d5c4bf14.png)

<나의 학습 모델>     
![image](https://user-images.githubusercontent.com/81483791/171209092-20e7f727-fedd-444c-b064-e850bc990ae6.png)

<참고한 블로그 학습 모델>

수정종가를 기준으로 MinMaxScaler 사용해서 데이터 정규화

2. 데이터 정규화

학습에 용이하게 하기 위해 
-1 ~ 1 사이로 값을 정규화함

## 실제 환경에 적용한다면?

예측가격이 실제가격을 1일만큼 옆으로 이동시킨 것과 비슷함     
![image](https://user-images.githubusercontent.com/81483791/171209194-6ae70899-2d1c-430a-92b8-6d3f4cf2b33c.png)

loss을 최소화 시키는 학습을 통해 모델은
내일 코인의 가격이 오늘 코인의 가격과 비슷할 것이다 라고 예측한 것과 같음

그러므로 실제로 적용 할 수 없음!
